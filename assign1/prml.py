# -*- coding: utf-8 -*-
"""prml.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tQ49qlGe_pclh4pbrVTi55-ILgbMZhRE
"""

import pandas as pd
import numpy as np
from sklearn.metrics import confusion_matrix
from collections import Counter

np.set_printoptions(suppress=True)

df = pd.read_csv('Dataset_1_Team_37.csv')
train = df.sample(frac=0.8)
test = df.drop(train.index)
N = len(train)
M = len(test)
classes = [0,1,2]
c = len(classes)
d = 2

# normalizing

x_1_mean = train['# x_1'].mean()
x_1_std = train['# x_1'].std()
x_2_mean = train['x_2'].mean()
x_2_std = train['x_2'].std()

train['# x_1'] = (train['# x_1']-x_1_mean)/x_1_std
train['x_2'] = (train['x_2']-x_2_mean)/x_2_std

train.head()

test['# x_1'] = (test['# x_1']-x_1_mean)/x_1_std
test['x_2'] = (test['x_2']-x_2_mean)/x_2_std

test.head()

train_data_0 = train.loc[train['Class_label'] == 0]
train_data_1 = train.loc[train['Class_label'] == 1]
train_data_2 = train.loc[train['Class_label'] == 2]

prior_prob = Counter(train['Class_label'])
for key in prior_prob:
  prior_prob[key] = prior_prob[key]/float(N)

prior_prob

loss_matrix = np.asarray([[0,2,1],[2,0,3],[1,3,0]])

def calc_f_value(mu,var,x):
  C = (2*np.pi*np.sqrt(np.linalg.det(var)))**(-1) 
  return C*np.exp(-0.5*np.inner((x-mu),np.matmul(np.linalg.pinv(var),(x-mu))))

def bayes_classifier(params=None,x=None,loss_matrix=loss_matrix,prior_prob=prior_prob):
  # x is 2d vector
  mu = params['mu']
  var = params['var']
  q = np.zeros(c)
  for i in classes:  
    for j in classes:
      q[i] += loss_matrix[i,j]*calc_f_value(mu[j],var[j],x)*prior_prob[j]
    
  return np.argmin(q)

"""### **Question 1a**"""

mu = np.asarray([[np.mean(train_data_0['# x_1']),np.mean(train_data_0['x_2'])],
                [np.mean(train_data_1['# x_1']),np.mean(train_data_1['x_2'])],
                [np.mean(train_data_2['# x_1']),np.mean(train_data_2['x_2'])]])
var = np.asarray([np.eye(d),np.eye(d),np.eye(d)])

params = {}
params['mu'] = mu
params['var'] = var

params

train_data = np.array(train[['# x_1','x_2']].apply(list,axis=1))
train_data = np.reshape(train_data.tolist(),(N,2))

predicted = np.zeros(len(train))
for i,x in enumerate(train_data):
  predicted[i] = int(bayes_classifier(params,x))

train['predicted'] = predicted
print(np.mean(train['Class_label'] == train['predicted']))
confusion_matrix(np.asarray(train['Class_label']),np.asarray(train['predicted']))

test_data = np.array(test[['# x_1','x_2']].apply(list,axis=1))
test_data = np.reshape(test_data.tolist(),(M,2))

predicted = np.zeros(len(test))
predicted_dict = {}
for i in classes:
  predicted_dict[i] = []
for i,x in enumerate(test_data):
  pred_class = int(bayes_classifier(params,x))
  predicted[i] = pred_class
  predicted_dict[pred_class].append(x)

for i in predicted_dict:
  predicted_dict[i] = np.reshape(np.asarray(predicted_dict[i]),(-1,2))
test['predicted'] = predicted
print(np.mean(test['Class_label'] == test['predicted']))
confusion_matrix(np.asarray(test['Class_label']),np.asarray(test['predicted']))

test.head()

# rescaling
def denormalize(x,std,mean):
  return std*x + mean

for i in predicted_dict:
  predicted_dict[i][:,0] = denormalize(predicted_dict[i][:,0],x_1_std,x_1_mean)
  predicted_dict[i][:,1] = denormalize(predicted_dict[i][:,1],x_2_std,x_2_mean)

import matplotlib.pyplot as plt

def plot_scatter(data=predicted_dict,xlabel=None,ylabel=None,figsize=(9,6),model=1,dataset_no=1):
  plt.figure(figsize=figsize)
  plt.title(r'Scatter plot of model {} for dataset {}'.format(model,dataset_no),size=15)
  plt.grid()
  plt.scatter(data[0][:,0], data[0][:,1], marker='+', c='k')
  plt.scatter(data[1][:,0], data[1][:,1], marker='.', c='k')
  plt.scatter(data[2][:,0], data[2][:,1], marker='1', c='k')
  plt.xlabel(xlabel=r'input feature x1',size=15)
  plt.ylabel(ylabel=r'input feature x2',size=15)
  plt.show()

plot_scatter()

def plot_decision_surface(data=predicted_dict,
                          params=params, 
                          xlabel=None, 
                          ylabel=None, 
                          figsize=(9,6), 
                          model=1,
                          dataset_no=1):
  x = np.linspace(-1.5,1.5,201)
  y = np.arange(-1.5,1.5,201)
  xx, yy = np.meshgrid(x,y)
  print(xx,yy)
  z = int(bayes_classifier(params,[xx,yy]))
  plt.title(r'Scatter plot of model {} for dataset {}'.format(model,dataset_no),size=15)
  # plt.grid()
  # plt.scatter(data[0][:,0], data[0][:,1], marker='+', c='k')
  # plt.scatter(data[1][:,0], data[1][:,1], marker='.', c='k')
  # plt.scatter(data[2][:,0], data[2][:,1], marker='1', c='k')
  # plt.xlabel(xlabel=r'input feature x1',size=15)
  # plt.ylabel(ylabel=r'input feature x2',size=15)
  plt.imshow(z)

plot_decision_surface()

"""### **Question 1b**"""

mu = np.asarray([[np.mean(train_data_0['# x_1']),np.mean(train_data_0['x_2'])],
                [np.mean(train_data_1['# x_1']),np.mean(train_data_1['x_2'])],
                [np.mean(train_data_2['# x_1']),np.mean(train_data_2['x_2'])]])
var = np.asarray([np.diag([np.var(train_data_0['# x_1']),np.var(train_data_0['x_2'])]),
                  np.diag([np.var(train_data_1['# x_1']),np.var(train_data_1['x_2'])]),
                  np.diag([np.var(train_data_2['# x_1']),np.var(train_data_2['x_2'])])])
var = np.mean(var,axis=0)

params = {}
params['mu'] = mu
params['var'] = np.array([var,var,var])

params

train_data = np.array(train[['# x_1','x_2']].apply(list,axis=1))
train_data = np.reshape(train_data.tolist(),(N,2))

predicted = np.zeros(len(train))
for i,x in enumerate(train_data):
  predicted[i] = int(bayes_classifier(params,x))

train['predicted'] = predicted
print(np.mean(train['Class_label'] == train['predicted']))
confusion_matrix(np.asarray(train['Class_label']),np.asarray(train['predicted']))

test_data = np.array(test[['# x_1','x_2']].apply(list,axis=1))
test_data = np.reshape(test_data.tolist(),(M,2))

predicted = np.zeros(len(test))
for i,x in enumerate(test_data):
  predicted[i] = int(bayes_classifier(params,x))

test['predicted'] = predicted
print(np.mean(test['Class_label'] == test['predicted']))
confusion_matrix(np.asarray(test['Class_label']),np.asarray(test['predicted']))

test_data[:,0] = denormalize(test_data[:,0],x_1_std,x_1_mean)
test_data[:,1] = denormalize(test_data[:,1],x_2_std,x_2_mean)
params['mu'][:,0] = denormalize(params['mu'][:,0],x_1_std,x_1_mean)
params['mu'][:,1] = denormalize(params['mu'][:,1],x_2_std,x_2_mean)

import matplotlib.pyplot as plt
plt.grid()
plt.scatter(test_data[:,0],test_data[:,1])
plt.scatter(params['mu'][:,0],params['mu'][:,1])
plt.show()

"""### **Question 1c**"""

mu = np.asarray([[np.mean(train_data_0['# x_1']),np.mean(train_data_0['x_2'])],
                [np.mean(train_data_1['# x_1']),np.mean(train_data_1['x_2'])],
                [np.mean(train_data_2['# x_1']),np.mean(train_data_2['x_2'])]])
var = np.asarray([np.diag([np.var(train_data_0['# x_1']),np.var(train_data_0['x_2'])]),
                  np.diag([np.var(train_data_1['# x_1']),np.var(train_data_1['x_2'])]),
                  np.diag([np.var(train_data_2['# x_1']),np.var(train_data_2['x_2'])])])

params = {}
params['mu'] = mu
params['var'] = var
params

train_data = np.array(train[['# x_1','x_2']].apply(list,axis=1))
train_data = np.reshape(train_data.tolist(),(N,2))

predicted = np.zeros(len(train))
for i,x in enumerate(train_data):
  predicted[i] = int(bayes_classifier(params,x))

train['predicted'] = predicted
print(np.mean(train['Class_label'] == train['predicted']))
confusion_matrix(np.asarray(train['Class_label']),np.asarray(train['predicted']))

test_data = np.array(test[['# x_1','x_2']].apply(list,axis=1))
test_data = np.reshape(test_data.tolist(),(M,2))

predicted = np.zeros(len(test))
for i,x in enumerate(test_data):
  predicted[i] = int(bayes_classifier(params,x))

test['predicted'] = predicted
print(np.mean(test['Class_label'] == test['predicted']))
confusion_matrix(np.asarray(test['Class_label']),np.asarray(test['predicted']))

test_data[:,0] = denormalize(test_data[:,0],x_1_std,x_1_mean)
test_data[:,1] = denormalize(test_data[:,1],x_2_std,x_2_mean)
params['mu'][:,0] = denormalize(params['mu'][:,0],x_1_std,x_1_mean)
params['mu'][:,1] = denormalize(params['mu'][:,1],x_2_std,x_2_mean)

import matplotlib.pyplot as plt
plt.grid()
plt.scatter(test_data[:,0],test_data[:,1])
plt.scatter(params['mu'][:,0],params['mu'][:,1])
plt.show()

"""### **Question 1d**"""

mu = np.zeros((c,d))
var = np.zeros((c,d,d))

train_data_0_ = np.array(train_data_0[['# x_1','x_2']].apply(list,axis=1))
train_data_0_ = np.reshape(train_data_0_.tolist(),(len(train_data_0),2))
mu[0] = np.mean(train_data_0_,axis=0)

train_data_0_ = train_data_0_ - mu[0]
for x in train_data_0_:
  var[0] += np.outer(x,x)
  
var[0] = var[0]/N
var[0]

train_data_1_ = np.array(train_data_1[['# x_1','x_2']].apply(list,axis=1))
train_data_1_ = np.reshape(train_data_1_.tolist(),(len(train_data_1),2))
mu[1] = np.mean(train_data_1_,axis=0)

train_data_1_ = train_data_1_ - mu[1]
for x in train_data_1_:
  var[1] += np.outer(x,x)
  
var[1] = var[1]/N
var[1]

train_data_2_ = np.array(train_data_2[['# x_1','x_2']].apply(list,axis=1))
train_data_2_ = np.reshape(train_data_2_.tolist(),(len(train_data_2),2))
mu[2] = np.mean(train_data_2_,axis=0)

train_data_2_ = train_data_2_ - mu[2]
for x in train_data_2_:
  var[2] += np.outer(x,x)
  
var[2] = var[2]/N
var[2]

params = {}
params['mu'] = mu
params['var'] = var
params

train_data = np.array(train[['# x_1','x_2']].apply(list,axis=1))
train_data = np.reshape(train_data.tolist(),(3600,2))

predicted = np.zeros(len(train))
for i,x in enumerate(train_data):
  predicted[i] = int(bayes_classifier(params,x))

train['predicted'] = predicted
print(np.mean(train['Class_label'] == train['predicted']))
confusion_matrix(np.asarray(train['Class_label']),np.asarray(train['predicted']))

test_data = np.array(test[['# x_1','x_2']].apply(list,axis=1))
test_data = np.reshape(test_data.tolist(),(M,2))

predicted = np.zeros(len(test))
for i,x in enumerate(test_data):
  predicted[i] = int(bayes_classifier(params,x))

test['predicted'] = predicted
print(np.mean(test['Class_label'] == test['predicted']))
confusion_matrix(np.asarray(test['Class_label']),np.asarray(test['predicted']))

"""# **Question 2**"""

import scipy.stats as stats
import matplotlib.pyplot as plt

df = pd.read_csv('Dataset_3_Team_37.csv')
data = df['# x_1'].values
N = data.shape[0]

np.mean(data)

mu_0 = -1
r = 0.1
mu = (N/(N+r))*np.mean(data) + (r/(N+r))*mu_0
var = np.mean((data-mu)**2)
sigma = np.sqrt(var)
sigma

def plot_gaussian(mu=0,sigma=1,xlabel=None,ylabel=None,rate=-1,n=-1,figsize=(9,6)):
  x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)
  plt.figure(figsize=figsize)
  plt.title(r'n = {}, $\sigma^2/\sigma_0^2 = {}, \mu = {:.4f}, \sigma = {:.4f}$'.format(n,rate,mu,sigma),size=15)
  plt.grid()
  plt.plot(x, stats.norm.pdf(x, mu, sigma))
  plt.xlabel(xlabel=r'x$\rightarrow$',size=15)
  plt.ylabel(ylabel=r'$y\rightarrow$',size=15)
  plt.savefig('image_'+str(n)+'_r_'+str(rate)+'.png')
  plt.show()

n = 100
r = 1
x = np.random.choice(data,size=n,replace=False)
mu_n = (n/(n+r))*np.mean(x) + (r/(n+r))*mu_0
sigma_n = sigma/np.sqrt(n+r)
plot_gaussian(mu=mu_n,sigma=sigma_n,xlabel='x',ylabel='y',rate=r,n=n)

list_n = [10,100,1000]
list_r = [0.1,1,10,100]
for n in list_n:
  x = np.random.choice(data,size=n,replace=False)
  for r in list_r:
    mu_n = (n/(n+r))*np.mean(x) + (r/(n+r))*mu_0
    sigma_n = sigma/np.sqrt(n+r)
    plot_gaussian(mu=mu_n,sigma=sigma_n,xlabel='x',ylabel='y',rate=r,n=n)

"""# **Question 6**"""

X = np.random.rand(100)
y = np.exp(np.sin(2*np.pi*X)) + np.random.normal(loc=0.0,scale=np.sqrt(0.2),size=X.shape[0])
plt.grid()
plt.scatter(X,y)
plt.show()

split = np.random.choice(X.shape[0],size=int(0.8*X.shape[0]),replace=False)
split_ = np.asarray([i for i in np.arange(X.shape[0]) if i not in split])
train_data_x = X[split]
train_data_y = y[split]
test_data_x = X[split_]
test_data_y = y[split_]

train_data = np.asarray(list((zip(train_data_x,train_data_y))))
test_data = np.asarray(list((zip(test_data_x,test_data_y))))
plt.scatter(train_data[:,0],train_data[:,1])
plt.show()

"""## **Question 6a**"""

idx = np.random.choice(train_data.shape[0],10,replace=False)
data = train_data[idx]
plt.scatter(data[:,0],data[:,1])
plt.show()

def poly_regression(data,deg=1):
  A = np.zeros((data.shape[0],deg+1))
  b = data[:,1]
  for i,x in enumerate(data[:,0]):
    A[i] = np.asarray([x**i for i in range(A.shape[1])])

  soln = np.linalg.lstsq(A,b,rcond=None)[0]
  y_hat = np.polyval(np.flip(soln),data[:,0])
  t_error = rmse(data[:,1],y_hat)
  return soln, t_error

def plot_poly_regression(data=data,soln=None):
  deg = soln.shape[0]-1
  x_ = np.linspace(0,1,101)
  y_hat = np.polyval(np.flip(soln),x_)

  plt.figure(figsize=(9,6))
  plt.title(r'no.of datapoints = {}, degree = {}'.format(data.shape[0],deg),size=15)
  plt.grid()
  plt.plot(x_,y_hat,color='red')
  plt.scatter(data[:,0],data[:,1])
  plt.xlabel(xlabel=r'x$\rightarrow$',size=15)
  plt.ylabel(ylabel=r'$\hat{y}\rightarrow$',size=15)
  plt.show()

def rmse(a,b):
  return np.sqrt(np.mean((a-b)**2))

sol1,_ = poly_regression(data,deg=1)
sol3,_ = poly_regression(data,deg=3)
sol6,_ = poly_regression(data,deg=6)
sol9,_ = poly_regression(data,deg=9)

plot_poly_regression(soln=sol1)
plot_poly_regression(soln=sol3)
plot_poly_regression(soln=sol6)
plot_poly_regression(soln=sol9)

"""## **Question 6b**"""

def overfit_test(test_data=test_data,train_data=train_data,deg=1):
  for n in [10,20,40,80]:
    idx = np.random.choice(train_data.shape[0],n,replace=False)
    data = train_data[idx]
    soln,train_error = poly_regression(data,deg=deg)
    y_hat_test = np.polyval(np.flip(soln),test_data[:,0])
    plt.figure(figsize=(9,6))
    plt.grid()
    plt.scatter(test_data[:,0],y_hat_test,label='predicted')
    plt.scatter(test_data[:,0],test_data[:,1],label='target')
    test_error = rmse(test_data[:,1],y_hat_test)
    plt.legend()
    print('n = {}, deg = {}, train_error = {}, test_error = {}'.format(n,deg,train_error,test_error))
    plt.show()

overfit_test(deg=6)

"""## **Question 6c**"""

def plot_target_output(test_data=test_data,train_data=train_data,deg=1):
    data = train_data
    soln,train_error = poly_regression(data,deg=deg)
    y_hat_test = np.polyval(np.flip(soln),test_data[:,0])
    plt.grid()
    plt.scatter(test_data[:,0],y_hat_test,label='predicted')
    plt.scatter(test_data[:,0],test_data[:,1],label='target')
    test_error = rmse(test_data[:,1],y_hat_test)
    plt.legend()
    print('n = {}, deg = {}, train_error = {}, test_error = {}'.format(data.shape[0],deg,train_error,test_error))
    plt.show()

plot_target_output(deg=9)

"""Degree 3 polynomial has the best generalization.

## **Question 6d**
"""

def find_rmse(train_data=train_data,test_data=test_data):
  train_errors,test_errors = [],[]
  for i in range(1,10):
    soln,train_error = poly_regression(data,deg=i)
    y_hat_test = np.polyval(np.flip(soln),test_data[:,0])
    test_error = rmse(test_data[:,1],y_hat_test)
    train_errors.append(train_error)
    test_errors.append(test_error)
  
  return train_errors, test_errors

train_errors, test_errors = find_rmse()

plt.figure(figsize=(9,6))
plt.title(r'RMSE with degree of polynomial',size=15)
plt.grid()
plt.plot(np.arange(1,10),train_errors,label='train_error')
plt.plot(np.arange(1,10),test_errors,label='test_error')
plt.xlabel(xlabel=r'degree of polynomial$\rightarrow$',size=15)
plt.ylabel(ylabel=r'RMSE$\rightarrow$',size=15)
plt.legend()
plt.show()