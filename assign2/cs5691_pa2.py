# -*- coding: utf-8 -*-
"""cs5691_pa2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qAUWP-eM5pxLUr3Q5NJJFrJj-lgK132v
"""

from google.colab import drive
drive.mount('/content/drive')

cd /content/drive/My Drive/codes/assign2/

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

"""# 1.1 Kernel Ridge Regression"""

data = pd.read_csv('Regression_dataset.csv')
data

y = data['Y']
data = data.drop(columns='Y')

X = data.loc[:,]
X

# X = StandardScaler().fit_transform(X)
X = np.asarray(X,dtype=np.float)
y = np.asarray(y,dtype=np.float)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
print(X_train.shape)
print(X_test.shape)

def linear_kernel(X,Y):
    assert X.shape[1] == Y.shape[1]
    K = np.matmul(X,Y.T)
    return K

def polynomial_kernel(X,Y,degree=2,gamma=None,a=1):
    assert X.shape[1] == Y.shape[1]
    if gamma is None:
        gamma = 1./X.shape[1]
    K = np.matmul(X,Y.T)
    K *= gamma
    K += a
    K **= degree
    return K

def KernelRidgeRegression(X_train,y_train,X_test,y_test,\
                   kernel='linear',\
                   degree=3,\
                   gamma=None,\
                   a=1,\
                   lambda_=1):
    if kernel == 'linear':
        K = linear_kernel(X_train,X_train)
    elif kernel == 'poly':
        K = polynomial_kernel(X_train,X_train,degree=degree,gamma=gamma,a=a)
    else:
        print('Invalid Kernel !')
        return
    alpha = np.matmul(np.linalg.inv(K+lambda_*np.eye(K.shape[0])),y_train)
    if kernel == 'linear':
        h = linear_kernel(X_train,X_test)
    elif kernel == 'poly':
        h = polynomial_kernel(X_train,X_test,degree=degree,gamma=gamma,a=a)

    y_preds = np.matmul(alpha.T,h)
    y_train_preds = np.matmul(alpha.T,K)
    return y_preds, np.linalg.norm(y_test-y_preds)**2/y_test.shape[0],np.linalg.norm(y_train-y_train_preds)**2/y_train.shape[0]

lambda_list = np.logspace(-4,4,9)
for lambda_ in lambda_list:
    y_pred, te_error, tr_error = KernelRidgeRegression(X_train,y_train,X_test,y_test,kernel='linear',lambda_=lambda_)
    print('lambda: {}, Test Error:{:.6f}, Train Error:{:.6f}'.format(lambda_,te_error, tr_error))

lambda_list = np.logspace(-4,4,9)
degree_list = np.arange(5)
for degree in degree_list:
    for lambda_ in lambda_list:
        y_pred, te_error, tr_error = KernelRidgeRegression(X_train,y_train,X_test,y_test,\
                                                    kernel='poly',lambda_=lambda_,degree=degree)
        print('degree: {}, lambda: {}, Test Error:{:.6f}, Train Error:{:.6f}'.format(degree,lambda_,te_error, tr_error))
    print('-'*50)

"""# Kernel SVM"""

data = pd.read_csv('Dataset_3_Team_32.csv')
data

y = data['Class_label']
X = data.drop(columns='Class_label')
X

import numpy as rnp

X = rnp.asarray(X,dtype=rnp.float)
y = rnp.asarray(y,dtype=rnp.float)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
print(X_train.shape)
print(X_test.shape)

from sklearn.svm import SVC
clf = SVC(kernel='linear')
clf.fit(X_train, y_train)
print(clf.score(X_train,y_train))
print(clf.score(X_test,y_test))

for i in range(10):
    clf = SVC(gamma='auto',kernel='poly',degree=i)
    clf.fit(X_train, y_train)
    print(i, clf.score(X_train,y_train), clf.score(X_test,y_test), clf.n_support_)

plt.scatter(data['# x_1'],data['x_2'])

"""# Kernel Logistic Regression"""

y_train = np.sign(y_train-0.5)

def sigmoid(x):
    return 1./(1+np.exp(-x))

def update_alpha(alpha,K,y):
    delta_alpha = np.zeros(K.shape[0])
    for i in range(K.shape[0]):
        delta_alpha += -y[i]*K[:,i]*sigmoid(-y[i]*np.matmul(alpha.T,K[:,i]))
    return delta_alpha

import jax.numpy as np
from jax import grad, jit

def loss_function(alpha,K,y,lambda_=1):
    J = np.log(1+np.exp(-y*np.matmul(alpha.T,K))).sum() + 0.5*lambda_*alpha.T.dot(K.dot(alpha))
    return J/K.shape[0]

grad_J = jit(grad(loss_function, argnums=0))

K = linear_kernel(X_train,X_train)
alpha = np.zeros(X_train.shape[0])
lr = 0.01
max_iters = 1000
for i in range(max_iters):
    # delta_alpha = update_alpha(alpha,K,y_train)
    J = loss_function(alpha,K,y_train)
    d_alpha = grad_J(alpha,K,y_train)
    alpha = alpha - lr*(0.9999)**i*d_alpha
    if (i+1) % 100 == 0:
      print('iters:{}, grad_norm:{}, loss:{}'.format(i+1,np.linalg.norm(d_alpha),J))

h = linear_kernel(X_train,X_test)
Z = np.matmul(alpha.T,h)

y_preds = np.asarray(sigmoid(Z) > 0.5,dtype=np.int32)
y_preds

y_train

y_train = (y_train+1)*0.5
np.mean(y_preds == y_test)

def predict_poly(alpha,X,Y,y,degree=2,gamma=None,a=1):
    h = polynomial_kernel(X,Y,degree=degree,gamma=gamma,a=a)
    # return np.sign(np.matmul((alpha*y).T,h))
    return np.asarray(sigmoid(np.matmul(alpha.T,h)) > 0.5,dtype=np.int32)

def predict_linear(alpha,X,Y,y):
    h = linear_kernel(X,Y)
    return np.sign(np.matmul((alpha*y).T,h))

def predict(alpha,X,Y,y,kernel='linear',degree=2,gamma=None,a=1):
    if kernel == 'linear':
        return predict_linear(alpha,X,Y,y)
    elif kernel == 'poly':
        return predict_poly(alpha,X,Y,y,degree=degree,gamma=gamma,a=a)
    else:
        print('Invalid Kernel !')
        return

import numpy as np
dataset = 3
def make_meshgrid(x, y, dataset=3):
    h = 0.02 if dataset == 3 else 0.75
    x_min, x_max = x.min() - 25*h, x.max() + 25*h
    y_min, y_max = y.min() - 25*h, y.max() + 25*h
#     print(x_min,x_max,y_min,y_max)
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    return xx, yy

# Set-up grid for plotting.
X0, X1 = X_train[:, 0], X_train[:, 1]
xx, yy = make_meshgrid(X0, X1,dataset=dataset)
X = np.c_[xx.ravel(), yy.ravel()]

h = linear_kernel(X_train,X)
Z = np.matmul(alpha.T,h)
y_preds = np.asarray(sigmoid(Z) > 0.5,dtype=np.int32)
y_preds_f = y_preds.reshape(xx.shape)
plt.figure(figsize=(9,6))
plt.contourf(xx,yy,y_preds_f,cmap=plt.cm.autumn)
plt.scatter(X0, X1, c=y_train, cmap=plt.cm.autumn, s=20, edgecolors='k',marker = 's')
plt.ylabel(r'$x_2\rightarrow$',size=15)
plt.xlabel(r'$x_1\rightarrow$',size=15)
# ax.set_xticks(())
# ax.set_yticks(())
plt.title('Decision surface and data points for Dataset 3 for linear kernel LR',size=15)
plt.show()

xx.shape

def plot_contours(ax,\
                  alpha_,\
                  X_train,\
                  y,\
                  xx,\
                  yy,\
                  kernel='poly',\
                  degree=2,\
                  gamma=None,\
                  a=1,\
                  **params):
    X = np.c_[xx.ravel(), yy.ravel()]
    Z = predict(alpha_,X_train,X,y,kernel=kernel,degree=degree,gamma=gamma,a=a)
    print(Z.shape)
    # Z = clf.predict(X)
    Z = Z.reshape(xx.shape)
    out = ax.contourf(xx, yy, Z, **params)
    return out

fig, ax = plt.subplots(figsize=(9,6))
# Set-up grid for plotting.
X0, X1 = X_train[:, 0], X_train[:, 1]
xx, yy = make_meshgrid(X0, X1,dataset=dataset)
degree, gamma, a = params
kernel = 'poly'
plot_contours(ax,\
              alpha_=alpha,\
              X_train=X_train,
              y=y_train,\
              xx=xx,\
              yy=yy,\
              kernel=kernel,\
              degree=degree,\
              gamma=gamma,\
              a=a,\
              cmap=plt.cm.autumn,\
              alpha=0.8)
ax.scatter(X0, X1, c=y_train, cmap=plt.cm.autumn, s=20, edgecolors='k',marker = 's')
# ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,
#                 facecolors='none', zorder=10, edgecolors='k')
ax.set_ylabel(r'$x_2\rightarrow$',size=15)
ax.set_xlabel(r'$x_1\rightarrow$',size=15)
# ax.set_xticks(())
# ax.set_yticks(())
ax.set_title('Decision surface and data points for Dataset 3 for polynomial kernel LR',size=15)
# ax.legend()
plt.show()

X_test[:10]

"""# Soft margin SVM"""

data = pd.read_csv('Dataset_2_Team_32.csv')
data

y = data['Class_label']
X = data.drop(columns='Class_label')
X

X = np.asarray(X,dtype=np.float)
y = np.asarray(y,dtype=np.float)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
print(X_train.shape)
print(X_test.shape)

from sklearn.svm import SVC
C_list = [10**i for i in range(-4,5)]
# C_list
for c in C_list:
    clf = SVC(kernel='linear',C=c)
    clf.fit(X_train, y_train)
    print('C:',c)
    print(clf.score(X_train,y_train))
    print(clf.score(X_test,y_test))
    print('-'*50)

from sklearn.metrics import plot_confusion_matrix
clf = SVC(kernel='linear',C=100000)
clf.fit(X_train, y_train)
plot_confusion_matrix(clf, X_test, y_test, values_format='.2g',cmap='gray')
plt.show()

w = clf.coef_[0]
a = -w[0] / w[1]
xx = np.linspace(-2.5, 2.5)
yy = a * xx - (clf.intercept_[0]) / w[1]

    # plot the parallels to the separating hyperplane that pass through the
    # support vectors (margin away from hyperplane in direction
    # perpendicular to hyperplane). This is sqrt(1+a^2) away vertically in
    # 2-d.
margin = 1 / np.sqrt(np.sum(clf.coef_ ** 2))
yy_down = yy - np.sqrt(1 + a ** 2) * margin
yy_up = yy + np.sqrt(1 + a ** 2) * margin

    # plot the line, the points, and the nearest vectors to the plane
plt.figure(0,figsize=(12,9))
plt.clf()
plt.plot(xx, yy, 'k-')
plt.plot(xx, yy_down, 'k--')
plt.plot(xx, yy_up, 'k--')

plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80,
                facecolors='none', zorder=10, edgecolors='k')
plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, zorder=10, cmap=plt.cm.Paired,
                edgecolors='k')

plt.axis('tight')
x_min = -1.5
x_max = 2.4
y_min = -3
y_max = 3

XX, YY = np.mgrid[x_min:x_max:200j, y_min:y_max:200j]
Z = clf.predict(np.c_[XX.ravel(), YY.ravel()])

    # Put the result into a color plot
Z = Z.reshape(XX.shape)
plt.figure(0,figsize=(9,6))
plt.title('Decision boundary plot for Dataset 2 for C = 1',size=15)
plt.xlabel(r'$x_1\rightarrow$',size=15)
plt.ylabel(r'$x_2\rightarrow$',size=15)
plt.pcolormesh(XX, YY, Z, cmap=plt.cm.autumn)
plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)
plt.show()

sv = clf.support_vectors_
wt = clf.coef_
b = clf.intercept_
np.sum(np.matmul(X_train,wt.T)+b >= 0.99)
# b

wt

zx = np.matmul(X_train,wt.T)+b
la = zx >= 0.95
lb = zx <= 1.05
np.sum(la*lb)

clf.n_support_

print(clf.predict(X_train)[:10])
print(clf.decision_function(X_train)[:10]) # np.matmul(X_train[:10],wt.T)+b

tr_res, te_res = [], []
x = [1,2,4,8,16]
for i in range(5):
    clf = SVC(kernel='linear',class_weight={0:2**i,1:1})
    clf.fit(X_train, y_train)
    tr_res.append(clf.score(X_train,y_train))
    te_res.append(clf.score(X_test,y_test))
plt.figure(0,figsize=(9,6))
plt.plot(x,tr_res,'-+',label='train_error')
plt.plot(x,te_res,'-*',label='test_error')
plt.title('Train and test accuracies vs k', size=15)
plt.xlabel(r'$k\rightarrow$', size=15)
plt.ylabel(r'$accuracy\rightarrow$',size=15)
plt.grid()
plt.legend()
plt.show()

"""#  Kernel Perceptron"""

import numpy as np

data = pd.read_csv('Dataset_1_Team_32.csv')
data

plt.scatter(data['# x_1'],data['x_2'])

data['Class_label'] = np.sign(data['Class_label']-0.5)
y = data['Class_label']
X = data.drop(columns='Class_label')
X

X = np.asarray(X,dtype=np.float)
y = np.asarray(y,dtype=np.float)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
print(X_train.shape,y_train.shape)
print(X_test.shape,y_test.shape)

def KernelPerceptron(X_train,y_train,X_test,y_test,\
                     max_iter=10,\
                     kernel='linear',\
                     degree=2,\
                     gamma=None,\
                     a=1):
    T = X_train.shape[0]
    alpha = np.zeros(T)
    if kernel == 'linear':
        for it in range(max_iter):
            up = 0
            for t in range(T):
                X_t = np.reshape(X_train[t],(1,2))
                h = linear_kernel(X_train,X_t)
                y_t_hat = np.sign(np.matmul((alpha*y_train).T,h))
                if y_train[t] != y_t_hat:
                    alpha[t] += 1
                    up += 1
            print('iter:{},updates:{}'.format(it+1,up))
        h = linear_kernel(X_train,X_test)
        y_test_preds = np.sign(np.matmul((alpha*y_train).T,h))
        h = linear_kernel(X_train,X_train)
        y_train_preds = np.sign(np.matmul((alpha*y_train).T,h))
    elif kernel == 'poly':
        for it in range(max_iter):
            up = 0
            for t in range(T):
                X_t = np.reshape(X_train[t],(1,2))
                h = polynomial_kernel(X_train,X_t,degree=degree,gamma=gamma,a=a)
                y_t_hat = np.sign(np.matmul((alpha*y_train).T,h))
                if y_train[t] != y_t_hat:
                    alpha[t] += 1
                    up += 1
            print('iter:{},updates:{}'.format(it+1,up))
        h = polynomial_kernel(X_train,X_test,degree=degree,gamma=gamma,a=a)
        y_test_preds = np.sign(np.matmul((alpha*y_train).T,h))
        h = polynomial_kernel(X_train,X_train,degree=degree,gamma=gamma,a=a)
        y_train_preds = np.sign(np.matmul((alpha*y_train).T,h))
    else:
        print('Invalid Kernel !')
        return
        
    return alpha, np.mean(y_test == y_test_preds), np.mean(y_train == y_train_preds), [degree, gamma, a]

def predict_poly(alpha,X,Y,y,degree=2,gamma=None,a=1):
    h = polynomial_kernel(X,Y,degree=degree,gamma=gamma,a=a)
    return np.sign(np.matmul((alpha*y).T,h))

def predict_linear(alpha,X,Y,y):
    h = linear_kernel(X,Y)
    return np.sign(np.matmul((alpha*y).T,h))

def predict(alpha,X,Y,y,kernel='linear',degree=2,gamma=None,a=1):
    if kernel == 'linear':
        return predict_linear(alpha,X,Y,y)
    elif kernel == 'poly':
        return predict_poly(alpha,X,Y,y,degree=degree,gamma=gamma,a=a)
    else:
        print('Invalid Kernel !')
        return

kernel = 'linear'
alpha,te_acc,tr_acc,params = KernelPerceptron(X_train,y_train,X_test,y_test,max_iter=50,kernel=kernel,degree=3)
print('test accuracy: {}, train accuracy: {}'.format(te_acc,tr_acc))

alpha.sum()

from sklearn.svm import SVC
clf = SVC(kernel='linear',C=1000,degree=3)
clf.fit(X_train, y_train)
print(clf.score(X_train, y_train))
print(clf.score(X_test, y_test))

np.linalg.norm(X_train,axis=1).max()

y_train.min()

w = clf.coef_
w1 = w/np.linalg.norm(w)
(y_train*np.matmul(w1,X_train.T)).min()

1 / np.sqrt(np.sum(clf.coef_ ** 2))

dataset = 1
def make_meshgrid(x, y, dataset=3):
    h = 0.02 if dataset == 3 else 0.75
    x_min, x_max = x.min() - 25*h, x.max() + 25*h
    y_min, y_max = y.min() - 25*h, y.max() + 25*h
#     print(x_min,x_max,y_min,y_max)
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    return xx, yy

def plot_contours(ax,\
                  alpha_,\
                  X_train,\
                  y,\
                  xx,\
                  yy,\
                  kernel='linear',\
                  degree=2,\
                  gamma=None,\
                  a=1,\
                  **params):
    X = np.c_[xx.ravel(), yy.ravel()]
    Z = predict(alpha_,X_train,X,y,kernel=kernel,degree=degree,gamma=gamma,a=a)
    # Z = clf.predict(X)
    Z = Z.reshape(xx.shape)
    out = ax.contourf(xx, yy, Z, **params)
    return out

fig, ax = plt.subplots(figsize=(9,6))
# Set-up grid for plotting.
X0, X1 = X_train[:, 0], X_train[:, 1]
xx, yy = make_meshgrid(X0, X1,dataset=dataset)
degree, gamma, a = params
plot_contours(ax,\
              alpha_=alpha,\
              X_train=X_train,
              y=y_train,\
              xx=xx,\
              yy=yy,\
              kernel=kernel,\
              degree=degree,\
              gamma=gamma,\
              a=a,\
              cmap=plt.cm.autumn,\
              alpha=1)
ax.scatter(X0, X1, c=y_train, cmap=plt.cm.autumn, s=20, edgecolors='k',marker = 's')
ax.set_ylabel(r'$x_2\rightarrow$')
ax.set_xlabel(r'$x_1\rightarrow$')
# ax.set_xticks(())
# ax.set_yticks(())
ax.set_title('Decison surface for polynomial kernel perceptron for dataset 1')
# ax.legend()
plt.show()